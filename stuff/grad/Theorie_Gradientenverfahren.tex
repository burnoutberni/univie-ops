\documentclass[a4paper, 11pt]{article}

\usepackage{tikz}
\usepackage{pgfplots}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage[T1]{fontenc}
\usepackage{float}

\begin{document}
\pgfplotsset{
  compat=1.8,
  colormap={whitered}{color(0cm)=(white); color(1cm)=(orange!75!red)}
}



\tableofcontents

\newpage
\section{Die Idee und der Zweck des Gradientenverfahrens}

Grundsätzlich ist das Gradientenverfahren ein Optimierungsverfahren, mit welchem mna das Minimum einer mehrdimensionalen Funktion errechnen kann. Der Gradient (${\nabla}f(x,y)$) einer Funktion \(f(x,y)\) zeigt, aus später gezeigten (siehe 4.2) Gründen, immer in die Richtung des steilsten Anstiegs, folglich zeigt -${\nabla}f(x,y)$ in die Richtung des stärksten Abstiegs. Das Gradientenverfahren macht sich dies zunutze und man wandert eine gewisse Dauer in diese Richtung, wie lange man in die errechnete Richtung wandert hängt auch davon ab, wie man z.B. die Schrittweite ermittelt, dazu später mehr.

In den folgenden Absätzen werde ich versuchen zwei verschiedene Arten des Gradientenverfahrens zu zeigen und zu erklären.

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}
\addplot3[surf]
{x*y^2};
\addplot3[->] coordinates {(-1.5,-0.5,54) (-2.5,-3.5,-70)}; 
\addplot3[only marks] coordinates {(-2,-2,-8)};
\end{axis}
\end{tikzpicture}
\caption[Gradientenvektor Beispiel]
{Hier zu sehen ist -$\vec{\nabla}f(-2,-2)$ der Funktion $f(x,y)=x*y^2$}
\end{figure}

\section{Schrittweitenbestimmung mittels Gleichungssystem}
In diesem Abschnitt möchte ich zeigen, wie man relative einfach (zu nicht zu komplizierten Funktionen) mittels dem Gradientenverfahren ein Minimum findet.

\subsection{Der Ablauf}
\begin{enumerate}

      \item 
      Es wird ein beliebiger Startpunk $P=(x,y)$ ausgewählt.
      \item 
      Es wird die Ableitung der Funktion $f(x,y)$, also ${\nabla}f(x)$ gebildet. Sollte dieser Gradient bereits den Wert null haben sind wir bereits fertig und haben ein lokales Minimum.
      \item 
      Der Punkt $P$ wird in den Gradienten eingesetzt und der negative Wert berechnet, also $-{\nabla}f(x_{P},y_{P})$.
      \item
      Um eine neuen Punkt $P^{[n+1]}$ zu finden stellen wir nun folgende Gleichung auf: $P^{[n+1]} = P^{[n]} + \alpha_{[n]} * s^{[n]}$. Nun haben wir die Richtung in die optimiert werden muss gefunden ($s^{[n]}$ ($=-{\nabla}f(x_{P},y_{P})$)), jetzt muss noch entschieden werden wie weit wir in diese Richtung "wandern" ($\alpha^{[n]}$, liegt zwischen $0$ und $1$) (Hinweis: $n$ ist die aktuelle Iteration). \\\\
      In diesem Kapitel werden wir die genau richtige Schrittweite auf Anhieb berechnen, dafür werden schaun wir uns zuerst die obere Gleichung $P^{[n+1]} = P^{[n]} + \alpha_{[n]} * s^{[n]}$ an. Aus dieser kann man herauslesen: $x=P^{[n]}_{x} + \alpha_{[n]} * s^{[n]}_{x}$ und $y=P^{[n]}_{y} + \alpha_{[n]} * s^{[n]}_{y}$. Dieses $x$ und $y$ wird nun in $f$ eingesetzt und die daraus resultierende Funktion, welche nur noch von $\alpha$ abhängig ist abermals abgeleitet. \\
      Diese abgeleitete Funktion wird nun gleich null gesetzt und so das passende $\alpha$ errechnet. Diese Art ein $alpha$ zu errechnen kann zwar bei einfachen Funktionen recht effizient von statten gehen, bei sehr komplexen und hochdimensionalen Funktionen aber auch enorm kompliziert werden, im gegensatz zu der in so einem Fall etwas simpleren Schrittweitenbestimmung mittels Armijo-Bedingung.      
      \item
      Wenn man einen neuen Punkt gefunden hat wird bei Punkt 3 fortgesetzt. Da es oft einen großen Rechenaufwand erfordert das genaue Minimum zu finden wird das Verfahren meist beendet wenn der Gradient einen akzeptablen Wert erreicht hat (So kann man als Abbruchbedingung z.B. die stärke des Anstiegs in einem Punkt (also den Betrag des Gradienten) nehmen).
      
      \end{enumerate}
      
      \subsection{Beispiel}
      
      Zu minimierende Funktion: $f(x,y) = y^4+2*x^2-3*x*y+1$, als Abbruchbedingung wählen wir $|{\nabla}f(x,y)| \leq 0.6$
      
\begin{enumerate}

	\item 
	Startpunkt $P=(1,2)$
	\item 
	Ableitung: ${\nabla}f(x,y)=
	\left( \begin{array}{c}
	4x-3y \\ 
	4y^3-3x 
	\end{array} \right)
	$($\rightarrow$ Der Gradient)
	\item
	$-{\nabla}f(1,1) =
	\left( \begin{array}{c}
	-4+6 \\ 
	-32+3 
	\end{array} \right)
	=
	\left( \begin{array}{c}
	2 \\ 
	-29
	\end{array} \right)
	$ ($\rightarrow$ Richtung des stärksten Abstiegs)
	
	\item
	Nun haben wir folgende Gleichung: $P^{[2]}=P^{[1]}+\alpha*-{\nabla}f(P^{[1]}_{x},P^{[1]}_{y})$ bzw. $P^{[2]}=\left( \begin{array}{c} 1 \\ 2 \end{array} \right) + \alpha * \left( \begin{array}{c} 2 \\ 29 \end{array} \right)$. Das heißt unser $x=1-2{\alpha}$ und unser $y=2-29{\alpha}$. Diese Werte werden nun in $f$ eingesetzt, sodass: $f({\alpha})=(2-29{\alpha})^4+2(1-2{\alpha})^2-3(2-29{\alpha})(1-2{\alpha})+1$. Das ergibt abgeleitet: $2829124{\alpha}^3-585336{\alpha}^2+40036{\alpha}-8370$. Wenn wir die abgeleitete Funktion gleich null setzen und die daraus resultierende Gleichung lösen bekommen wir ein passendes Alpha: ${\alpha}=0.039$
	
	$P^{[2]}=\left( \begin{array}{c} 1 \\ 2 \end{array} \right) 0.039 * \left( \begin{array}{c} 2 \\ -29 \end{array} \right) = \left( \begin{array}{c} 1.078 \\ 0.869 \end{array} \right)$

Das heißt unser neuer Punkt ist: $P^{[2]}=(1.078,0.869)$
	
	\item
	$-{\nabla}f(1.078,0.869) = \left( \begin{array}{c} -1.705 \\  0.609 \end{array} \right)$
	
	\item
	$P^{[3]}=\left( \begin{array}{c} 1.078 \\ 0.869 \end{array} \right) + \alpha * \left( \begin{array}{c} -1.705 \\ 0.609 \end{array} \right)$\\ $\rightarrow$ $x=1.078-1.705{\alpha}, y=0.869+0.609{\alpha}$\\ $\rightarrow$ $f({\alpha})=(0.869+0.609{\alpha})^4+2(1.078-1.705{\alpha})^2-3(0.869+0.609{\alpha})(1.078-1.705{\alpha})+1$\\ $\rightarrow$ $f'(\alpha)=\frac{-819485690419+5304767100523{\alpha}+588834041103{\alpha}^2+137552716161{\alpha}^3}{250000000000}$\\ $\rightarrow$ $\alpha=0.152$	
	
	\item
	$P^{[3]}=\left( \begin{array}{c} 1.078 \\ 0.869 \end{array} \right) 0.152 * \left( \begin{array}{c} -1.705 \\ 0.609 \end{array} \right) = \left( \begin{array}{c} 0.819\\ 0.962 \end{array} \right)$
	
	\item
	$-{\nabla}f(0.819,0.962) = \left( \begin{array}{c} -0.389 \\ -1.104 \end{array} \right)$
	
	\item
	$P^{[4]}=\left( \begin{array}{c} 0.819 \\ 0.962 \end{array} \right) + \alpha * \left( \begin{array}{c} -0.389 \\ -1.104 \end{array} \right)$\\ $\rightarrow$ $x=0.819-0.389{\alpha}, y=0.962-1.104{\alpha}$\\ $\rightarrow$ $f(\alpha)=(0.962-1.104{\alpha})^4+2(0.819-0.389{\alpha})^2-3(0.962-1.104{\alpha})(0.819-0.389{\alpha})+1$\\ $\rightarrow$ $f'(\alpha)=\frac{46422263808*x^3-121353852672*x^2+90342964466*x-10708170291}{7812500000}$\\ $\rightarrow$ $\alpha=0.145$
	
	\item
	$P^{[4]}=\left( \begin{array}{c} 0.819 \\ 0.962 \end{array} \right) 0.145 * \left( \begin{array}{c} -0.389 \\ -1.104 \end{array} \right) = \left( \begin{array}{c} 0.763 \\ 0.802 \end{array} \right)$
	
	\item
	$-{\nabla}f(0.763,0.802) = \left( \begin{array}{c} -0.646 \\ 0.226 \end{array} \right)$
	
	\item
	$P^{[5]}=\left( \begin{array}{c} 0.763 \\ 0.802 \end{array} \right) + \alpha * \left( \begin{array}{c} -0.646 \\ 0.226 \end{array} \right)$\\ $\rightarrow$ $x=0.763-0.646{\alpha}, y=0.802+0.226{\alpha}$\\ $\rightarrow$ $f(\alpha)=(0.802+0.226{\alpha})^4+2(0.763-0.646{\alpha})^2-3(0.802+0.226{\alpha})(0.763-0.646{\alpha})+1$\\ $\rightarrow$ $f'(\alpha)=\frac{(-7317218037+45929178907*x+1735805091*x^2+163047361*x^3)}{15625000000}$\\ $\rightarrow$ $\alpha=0.158$
	
	\item
	$P^{[5]}=\left( \begin{array}{c} 0.763 \\ 0.802 \end{array} \right) 0.158 * \left( \begin{array}{c} -0.646 \\ 0.226 \end{array} \right) = \left( \begin{array}{c} 0.661 \\0.838 \end{array} \right)$
	
	
	
		
An dieser Stelle werde ich aufhöhren weitere Iterationen durchzuführen, da man erkennt wie das Verfahren funktioniert und aufgrund des Zick-Zack Kurses noch viele Iterationen nötig wären. An dieser Funktion erkennt man die größte Schwäche des Gradientenverfahrens, es nähert sich anfangs zwar recht schnell, später jedoch nur recht langsam im Zick-Zack Kurs dem Optimum.\\
      
\end{enumerate}

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[
	scale=2,
    3d box,
    enlargelimits=false,
    grid=major,
    samples=21,
    domain=0.4:2,
    xmax=1.2,
    zmax=14,
    xlabel={$x$},
    ylabel={$y$}
]
\addplot3[surf]
{y^4+2*x^2-3*x*y+1};
\addplot3 [contour gnuplot = {number=20, labels={false}, draw color=black},
        samples=21]
        {y^4+2*x^2-3*x*y+1};
\addplot3 [contour gnuplot = {number=20, labels={false}, draw color=black},
        samples=21,z filter/.code={\def\pgfmathresult{14}}]
        {y^4+2*x^2-3*x*y+1};
\node[label={180:{$P^{[1]}$}},circle,fill,inner sep=2pt] at (axis cs:1,2,13) {};
\node[label={180:{$P^{[2]}$}},circle,fill,inner sep=2pt] at (axis cs:1.078,0.869,1.084090135921) {};
\node[label={180:{$P^{[3]}$}},circle,fill,inner sep=2pt] at (axis cs:0.819,0.962,0.8343345971359999) {};
\node[label={180:{$P^{[4]}$}},circle,fill,inner sep=2pt] at (axis cs:0.763,0.802,0.7422713856160001) {};
\node[label={180:{$P^{[4]}$}},circle,fill,inner sep=2pt] at (axis cs:0.661,0.838,0.7052346355360002) {};


\addplot3[->, line width=2pt] coordinates {(1,2,14) (1.078,0.869,14)}; 
\addplot3[->, line width=2pt] coordinates {(1.078,0.869,14) (0.819,0.962,14)};
\addplot3[->, line width=2pt] coordinates {(0.819,0.962,14) (0.763,0.802,14)}; 
\addplot3[->, line width=2pt] coordinates {(0.763,0.802,14) (0.661,0.838,14)};

\node[label={180:{$P^{[1]}$}},circle,fill,inner sep=2pt] at (axis cs:1,2,14) {};
\node[label={180:{$P^{[2]}$}},circle,fill,inner sep=2pt] at (axis cs:1.078,0.869,14) {};
\node[label={180:{$P^{[3]}$}},circle,fill,inner sep=2pt] at (axis cs:0.819,0.962,14) {}; 
\node[label={180:{$P^{[4]}$}},circle,fill,inner sep=2pt] at (axis cs:0.763,0.802,14) {};
\node[label={180:{$P^{[5]}$}},circle,fill,inner sep=2pt] at (axis cs:0.661,0.838,14) {};

\end{axis}

\end{tikzpicture}
\caption{Hier zu sehen ist die Zweidimensionale Funktion, die bei der Angabe von Beispiel 1 gegeben ist. Zusätzlich sind hier die Höhenschichtlinien eingezeichnet. Wie man sieht nähert man sich mit jeder Iteration an das Optimum der Funktion an.}
\end{figure}

\section{Schrittweitenbestimmung mittels der Armijo-Bedingung}
\subsection{Der Ablauf}

\begin{enumerate}
      \item 
      Es wird ein beliebiger Startpunk $P=(x,y)$ ausgewählt.
      \item 
      Es wird die Ableitung der Funktion $f(x,y)$, also ${\nabla}f(x)$ gebildet. Sollte dieser Gradient bereits den Wert null haben sind wir bereits fertig und haben ein lokales Minimum.
      \item 
      Der Punkt $P$ wird in den Gradienten eingesetzt und der negative Wert berechnet, also $-{\nabla}f(x_{P},y_{P})$.
      \item
      Um eine neuen Punkt $P^{[n+1]}$ zu finden stellen wir nun folgende Gleichung auf: $P^{[n+1]} = P^{[n]} + \alpha_{[n]} * s^{[n]}$. Nun haben wir die Richtung in die optimiert werden muss gefunden ($s^{[n]}$ ($=-{\nabla}f(x_{P},y_{P})$)), jetzt muss noch entschieden werden wie weit wir in diese Richtung "wandern" ($\alpha^{[n]}$, liegt zwischen $0$ und $1$) (Hinweis: $n$ ist die aktuelle Iteration). \\\\
      Das kann man einerseits mit Hilfe eines fixen Werts machen, andererseits auch mit einer Folge (wie z.B $\frac{1}{\sqrt{n}}$) oder einer jedes mal neu errechneten Schrittweite. Zu beachten ist, dass die ausgewählte Folge auf jeden Fall divergent sein muss, da man sonst bis zu einer maximalen Entfernung "wandern" kann und die Wahrscheinlichkeit groß ist das Optimum nie zu erreichen. Sollte man jedes mal einen fixen Wert verliert man zwar den Aufwand für die Berechnung der  Schrittweite, jedoch ist die Wahrscheinlichkeit hoch, dass man viele Iterationen benötigt oder in einer endlosen Schleife festhängt. \\\\
      Es gibt verschieden Arten wie man die Schrittweite berechnen kann, im Folgenden werde ich die Schrittweitenbestimmung nach Armijo erklären. Dieses Verfahren ist eines der sogenannten "line-search" Verfahren, das von mir gezeigte ist jedoch kein exaktes line-search Verfahren sondern lediglich eine einfache Heuristik. \\\\
      \textbf{Die Armijo-Bedingung:} $\varphi(\alpha) \leq \varphi(0) + c * \alpha^{[n]} * \varphi(0)'$. \\\\
      Wobei $c$ eine Konstante zwischen null und eins ist und dazu dient das die Bedingung nicht zu restriktiv ist, $\varphi(\alpha) = f(P^{[n]} + \alpha^{[n]}s^{[n]})$ und $\varphi(0)' = {\nabla}f(P^{n}) * s^{[n]}$
      \item
      Für den Anfang wird $\alpha=1$ gewählt und errechnet ob die Armijo-Bedingung erfüllt ist - sollte sie erfüllt sein haben wir ein passende $\alpha$ gefunden, wenn nicht wird $\alpha$ mit einem $\beta$, welches ebenfalls zwischen null und eins liegt, multipliziert und wieder geprüft ob die Ungleichung erfüllt ist. (Hinweis: In der Praxis liefern die Werte $c = 0.01$ und $\beta = 0.9$ häufig gute Ergebnisse.
      \item
      Wenn man einen neuen Punkt gefunden hat wird bei Punkt 3 fortgesetzt.
      
      
\end{enumerate}

\subsection{Beispiel}

Zu minimierende Funktion: $f(x,y)=y^4+2x^2-3xy+1$, als Abbruchbedingung wählen wir $|{\nabla}f(x,y)| \leq 0.3$
\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[
	scale=2,
    3d box,
    enlargelimits=false,
    grid=major,
    samples=21,
    domain=0.2:1.1,
    view={15}{30},
    xlabel={$x$},
    ylabel={$y$}
]
\addplot3[surf]
{ y^4+2*x^2-3*x*y+1 };
\addplot3 [contour gnuplot = {number=20, labels={false}, draw color=black},
        samples=21,z filter/.code={\def\pgfmathresult{3}}]
        {y^4+2*x^2-3*x*y+1};
\addplot3 [contour gnuplot = {number=20, labels={false}, draw color=black},
        samples=21]
        {y^4+2*x^2-3*x*y+1};
\node[label={180:{$P^{[1]}$}},circle,fill,inner sep=2pt] at (axis cs:1,1,1) {};
\node[label={180:{$P^{[2]}$}},circle,fill,inner sep=2pt] at (axis cs:0.41,0.41,0.86) {};
\node[label={180:{$P^{[3]}$}},circle,fill,inner sep=2pt] at (axis cs:0.308,0.648,0.767) {};
\node[label={180:{$P^{[4]}$}},circle,fill,inner sep=2pt] at (axis cs:0.486,0.607,0.723) {};
\node[label={180:{$P^{[5]}$}},circle,fill,inner sep=2pt] at (axis cs:0.455,0.748,0.706) {};
\node[label={180:{$P^{[1]}$}},circle,fill,inner sep=2pt] at (axis cs:1,1,3) {};
\addplot3[->, line width=2pt] coordinates {(1,1,3) (0.41,0.41,3)}; 
\node[label={180:{$P^{[2]}$}},circle,fill,inner sep=2pt] at (axis cs:0.41,0.41,3) {};
\addplot3[->, line width=2pt] coordinates {(0.41,0.41,3) (0.308,0.648,3)}; 
\node[label={180:{$P^{[3]}$}},circle,fill,inner sep=2pt] at (axis cs:0.308,0.648,3) {};
\addplot3[->, line width=2pt] coordinates {(0.308,0.648,3) (0.486,0.607,3)}; 
\node[label={180:{$P^{[4]}$}},circle,fill,inner sep=2pt] at (axis cs:0.486,0.607,3) {};
\addplot3[->, line width=2pt] coordinates {(0.486,0.607,3) (0.455,0.748,3)}; 
\node[label={180:{$P^{[5]}$}},circle,fill,inner sep=2pt] at (axis cs:0.455,0.748,3) {};
\end{axis}
\end{tikzpicture}
\caption[Zweidimensionale Funktion für Beispiel 2]
{Hier zu sehen ist die Zweidimensionale Funktion, die bei der Angabe von Beispiel 2 gegeben ist. Zusätzlich sind hier die Höhenschichtlinien eingezeichnet. Wie man sieht nähert man sich mit jeder Iteration an das Optimum der Funktion an. Im Vergleich zur Abbildung 2 erkennt man, dass es hier durchaus auch vorkommen kann, dass man über das Ziel hinausschießt und kein optimales $\alpha$ hat.}
\end{figure}


\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[
    3d box,
    enlargelimits=false,
    grid=major,
    samples=21,
    domain=0.2:1.1,
    view={0}{90},
    xlabel={$x$},
    ylabel={$y$},
    scale=2
]
\addplot3 [contour gnuplot = {number=100, labels={false}, draw color=black},
        samples=21,z filter/.code={\def\pgfmathresult{0}}]
        {y^4+2*x^2-3*x*y+1};
\node[label={180:{$P^{[1]}$}},circle,fill,inner sep=2pt] at (axis cs:1,1,0) {};
\addplot3[->, line width=2pt] coordinates {(1,1,0) (0.41,0.41,0)}; 
\node[label={180:{$P^{[2]}$}},circle,fill,inner sep=2pt] at (axis cs:0.41,0.41,0) {};
\addplot3[->, line width=2pt] coordinates {(0.41,0.41,0) (0.308,0.648,0)}; 
\node[label={180:{$P^{[3]}$}},circle,fill,inner sep=2pt] at (axis cs:0.308,0.648,0) {};
\addplot3[->, line width=2pt] coordinates {(0.308,0.648,0) (0.486,0.607,0)}; 
\node[label={180:{$P^{[4]}$}},circle,fill,inner sep=2pt] at (axis cs:0.486,0.607,0) {};
\addplot3[->, line width=2pt] coordinates {(0.486,0.607,0) (0.455,0.748,0)}; 
\node[label={180:{$P^{[5]}$}},circle,fill,inner sep=2pt] at (axis cs:0.455,0.748,0) {};
\end{axis}
\end{tikzpicture}
\caption[Höhenlinien der zweidimensionale Funktion für Beispiel 1]
{Hier zu sehen sind die Höhenlinien der Funktion bei Beispiel 1. Wie man sieht nähert man sich mit jeder Iteration an das Optimum der Funktion an und der Gradient steht immer rechtwinklig auf den Höhenlinien (Aber im Gegensatz zum "normalen" Gradientenverfahren bei Beispiel 1 stehen die Gradienten nicht normal aufeinander).}
\end{figure}

\begin{enumerate}
	\item 
	Startpunkt $P=(1,1)$
	\item 
	Ableitung: ${\nabla}f(x,y)=
	\left( \begin{array}{c}
	4x-3y \\ 
	4y^3-3x 
	\end{array} \right)
	$($\rightarrow$ Der Gradient)
	\item
	$-{\nabla}f(1,1) =
	\left( \begin{array}{c}
	-4+3 \\ 
	-4+3 
	\end{array} \right)
	=
	\left( \begin{array}{c}
	-1 \\ 
	-1
	\end{array} \right)
	$ ($\rightarrow$ Richtung des stärksten Abstiegs)
	\item
	Die Armijo-Bedingung (mit $\alpha = 1$, $c = 0.1$ und $\beta = 0.9$):\\
	$f( \left( \begin{array}{c} 1 \\ 1 \end{array} \right) + 1 * \left( \begin{array}{c} -1 \\ -1 \end{array} \right)) \leq f(\left( \begin{array}{c} 1 \\ 1 \end{array} \right)) + 0.1 * 1 *  \left( \begin{array}{c} 1 \\ 1 \end{array} \right) * \left( \begin{array}{c} -1 \\ -1 \end{array} \right)$ \\
	$= 1 \leq 0.8$ \\
	$\rightarrow \alpha^{[2]} = \alpha^{[1]} * \beta = 0.9$ \\
	
	$f( \left( \begin{array}{c} 1 \\ 1 \end{array} \right) + 0.9 * \left( \begin{array}{c} -1 \\ -1 \end{array} \right)) \leq f(\left( \begin{array}{c} 1 \\ 1 \end{array} \right)) + 0.1 * 0.9 *  \left( \begin{array}{c} 1 \\ 1 \end{array} \right) * \left( \begin{array}{c} -1 \\ -1 \end{array} \right)$ \\
	$= 0.99 \leq 0.82$ \\
	$\rightarrow \alpha^{[3]} = \alpha^{[2]} * \beta = 0.81$ \\
	
	$f( \left( \begin{array}{c} 1 \\ 1 \end{array} \right) + 0.81 * \left( \begin{array}{c} -1 \\ -1 \end{array} \right)) \leq f(\left( \begin{array}{c} 1 \\ 1 \end{array} \right)) + 0.1 * 0.81 *  \left( \begin{array}{c} 1 \\ 1 \end{array} \right) * \left( \begin{array}{c} -1 \\ -1 \end{array} \right)$ \\
	$= 0.965 \leq 0.838$ \\
	$\rightarrow \alpha^{[4]} = \alpha^{[3]} * \beta = 0.729$ \\
	
	$f( \left( \begin{array}{c} 1 \\ 1 \end{array} \right) + 0.729 * \left( \begin{array}{c} -1 \\ -1 \end{array} \right)) \leq f(\left( \begin{array}{c} 1 \\ 1 \end{array} \right)) + 0.1 * 0.729 *  \left( \begin{array}{c} 1 \\ 1 \end{array} \right) * \left( \begin{array}{c} -1 \\ -1 \end{array} \right)$ \\
	$= 0.932 \leq 0.854$ \\
	$\rightarrow \alpha^{[5]} = \alpha^{[4]} * \beta = 0.656$ \\
	
	$f( \left( \begin{array}{c} 1 \\ 1 \end{array} \right) + 0.656 * \left( \begin{array}{c} -1 \\ -1 \end{array} \right)) \leq f(\left( \begin{array}{c} 1 \\ 1 \end{array} \right)) + 0.1 * 0.656 *  \left( \begin{array}{c} 1 \\ 1 \end{array} \right) * \left( \begin{array}{c} -1 \\ -1 \end{array} \right)$ \\
	$= 0.896 \leq 0.869$ \\
	$\rightarrow \alpha^{[6]} = \alpha^{[5]} * \beta = 0,59$ \\
	
	$f( \left( \begin{array}{c} 1 \\ 1 \end{array} \right) + 0,59 * \left( \begin{array}{c} -1 \\ -1 \end{array} \right)) \leq f(\left( \begin{array}{c} 1 \\ 1 \end{array} \right)) + 0.1 * 0,59 *  \left( \begin{array}{c} 1 \\ 1 \end{array} \right) * \left( \begin{array}{c} -1 \\ -1 \end{array} \right)$ \\
	$= 0.86 \leq 0.882$ \\
	Nun haben wir ein passendes $\alpha$ gefunden und können zum nächsten Punkt wandern.
	
	\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[
	grid=major,
	domain=0:1,
	ymin=0.6,
    ymax=1.2,
    xlabel={$\alpha$},
    ylabel={$\varphi(\alpha)$},
    scale=2
]
\addplot[]
{ (1-x)^4+2*(1-x)^2-3*(1-x)*(1-x)+1 } node[above,pos=0.4] {$\varphi(\alpha)$};
\addplot[color=red]
{ (1^4+2*1^2-3*1*1+1) + x * (-2)} node[at=(current axis.left of origin), anchor=west, pos=0.15] {$\varphi(0) + \alpha^{[n]} * \varphi(0)'$};
\addplot[color=blue]
{ (1^4+2*1^2-3*1*1+1) + 0.1 * x * (-2)} node[at=(current axis.left of origin), anchor=south, pos=0.3, yshift=15pt] {$\varphi(0) +  c * \alpha^{[n]} * \varphi(0)'$};

\node[label={180:{$\alpha=0.59$}},circle,fill,inner sep=2pt] at (axis cs:0.59,0.86) {};
\node[label={180:{$\alpha=0.656$}},circle,fill,inner sep=2pt] at (axis cs:0.656,0.896,0) {};
\node[label={180:{$\alpha=0.729$}},circle,fill,inner sep=2pt] at (axis cs:0.729,0.932,0) {};
\node[label={180:{$\alpha=0.81$}},circle,fill,inner sep=2pt] at (axis cs:0.81,0.965) {};
\node[label={180:{$\alpha=0.9$}},circle,fill,inner sep=2pt] at (axis cs:0.9,0.99) {};
\node[label={180:{$\alpha=1$}},circle,fill,inner sep=2pt] at (axis cs:1,1,0) {};

\end{axis}
\end{tikzpicture}
\caption[Die Armijo-Bedingung]
{Hier sieht man die Funktion $\varphi(\alpha)=(1-\alpha)^4+2*(1-\alpha)^2-3*(1-\alpha)*(1-\alpha)+1)$, sie ist sozusagen ein Durchschnitt durch die bei Beispiel 1 gegebene Funktion. Diese Funktion wird erzeugt, wenn man vom Punkt $P^{[1]}=(1,1)$ in Richtung des Gradienten wandert. Die blaue Gerade zeigt die Armijo-Bedingung bei $c=0.1$, die rote bei $c=1$ bzw. ohne c.}
\end{figure}

    \item
    $P^{[2]} = \left( \begin{array}{c} 1 \\ 1 \end{array} \right) + 0,59 * \left( \begin{array}{c} -1 \\ -1 \end{array} \right) = \left( \begin{array}{c} 0.41 \\ 0.41 \end{array} \right)$
    \item
    $-{\nabla}f(0.41,0.41) = \left( \begin{array}{c} -0.41 \\ 0.95 \end{array} \right)$, \\
	$|{\nabla}f(0.41,0.41)| = \sqrt{0.41^2+0.95^2} = 1.071$
	\item    
	Die Armijo-Bedingung (mit $\alpha = 1$, $c = 0.1$ und $\beta = 0.5$):\\
	$f( \left( \begin{array}{c} 0.41 \\ 0.41 \end{array} \right) + 1 * \left( \begin{array}{c} -0.41 \\ 0.95 \end{array} \right)) \leq f(\left( \begin{array}{c} 0.41 \\ 0.41 \end{array} \right)) + 0.1 * 1 *  \left( \begin{array}{c} -0.41 \\ 0.95 \end{array} \right) * \left( \begin{array}{c} 0.41 \\ -0.95 \end{array} \right)$ \\
	$= 4.421 \leq 0.753$
	$\rightarrow \alpha^{[2]} = \alpha^{[1]} * \beta = 0.5$ \\
	$\rightarrow = 1.153 \leq 0.807$
	$\rightarrow \alpha^{[3]} = \alpha^{[2]} * \beta = 0.25$ \\
	$\rightarrow = 0.768 \leq 0.833$
	\item
	$P^{[3]} = \left( \begin{array}{c} 0.41 \\ 0.41 \end{array} \right) + 0.25 * \left( \begin{array}{c} -0.41 \\ 0.95 \end{array} \right) = \left( \begin{array}{c} 0.308 \\ 0.648 \end{array} \right)$
	\item
	$-{\nabla}f(0.308,0.648) = \left( \begin{array}{c} 0.712 \\ -0.164 \end{array} \right)$, \\
	$|{\nabla}f(0.308,0.648)| = \sqrt{0.712^2+0.164^2} = 0.534$
	\item
	Die Armijo-Bedingung (mit $\alpha = 1$, $c = 0.1$ und $\beta = 0.5$):\\
	$f( \left( \begin{array}{c} 0.308 \\ 0.648 \end{array} \right) + 1 * \left( \begin{array}{c} 0.712 \\ -0.164 \end{array} \right)) \leq f(\left( \begin{array}{c} 0.308 \\ 0.648 \end{array} \right)) + 0.1 * 1 *  \left( \begin{array}{c} 0.712 \\ -0.164 \end{array} \right) * \left( \begin{array}{c} -0.712 \\ 0.164 \end{array} \right)$ \\
	$= 1.655 \leq 0.714$
	$\rightarrow \alpha^{[2]} = \alpha^{[1]} * \beta = 0.5$ \\
	$\rightarrow = 0.857 \leq 0.741$
	$\rightarrow \alpha^{[3]} = \alpha^{[2]} * \beta = 0.25$ \\
	$\rightarrow = 0.723 \leq 0.754$
	\item
	$P^{[4]} = \left( \begin{array}{c} 0.308 \\ 0.648 \end{array} \right) + 0.25 * \left( \begin{array}{c} 0.712 \\ -0.164 \end{array} \right) = \left( \begin{array}{c} 0.486 \\ 0.607 \end{array} \right)$
	\item
	$-{\nabla}f(0.486,0.607) = \left( \begin{array}{c} -0.123 \\ 0.563. \end{array} \right)$, \\
	$|{\nabla}f(0.486,0.607)| = \sqrt{0.123^2+0.563^2} = 0.332$ 
	\item
	Die Armijo-Bedingung (mit $\alpha = 1$, $c = 0.1$ und $\beta = 0.5$):\\
	$f( \left( \begin{array}{c} 0.486 \\ 0.607 \end{array} \right) + 1 * \left( \begin{array}{c} -0.123 \\ 0.563 \end{array} \right)) \leq f(\left( \begin{array}{c} 0.486 \\ 0.607 \end{array} \right)) + 0.1 * 1 *  \left( \begin{array}{c} -0.123 \\ 0.563 \end{array} \right) * \left( \begin{array}{c} 0.123 \\ -0.563 \end{array} \right)$ \\
	$= 1.863 \leq 0.689$
	$\rightarrow \alpha^{[2]} = \alpha^{[1]} * \beta = 0.5$ \\
	$\rightarrow = 0.852 \leq 0.707$
	$\rightarrow \alpha^{[3]} = \alpha^{[2]} * \beta = 0.25$ \\
	$\rightarrow = 0.706 \leq 0.715$
	\item
	$P^{[5]} = \left( \begin{array}{c} 0.486 \\ 0.607 \end{array} \right) + 0.25 * \left( \begin{array}{c} -0.123 \\ 0.563 \end{array} \right) = \left( \begin{array}{c} 0.455 \\ 0.748 \end{array} \right)$
    \item
	$-{\nabla}f(0.455,0.748) = \left( \begin{array}{c} 0.424 \\ -0.309. \end{array} \right)$, \\
	$|{\nabla}f(0.455,0.748)| = \sqrt{0.424^2+0.309^2} = 0.275$ \\
	Die Abbruchbedingung ist erfüllt und somit ist ein Punkt, welcher nahe genug am Optimum liegt gefunden.
	
\end{enumerate}

\section{Weitere Theorie}
\subsection{Die Ungleichung von  Cauchy-Schwarz}
Die Ungleichung von Chauchy-Schwarz wird uns für die folgenden theoretischen Ausarbeitungen die Basis lierfern.

$|\langle x,y \rangle|^2 \leq \langle x, x\rangle \cdot \langle y,y\rangle$ bzw. $\left|\langle x,y \rangle\right| \leq \|x\| \cdot \| y\|$

Das heißt das der Betrag der des Skalarproduktes der Vektoren $x$ und $y$ immer kleiner gleich dem Produkt der Längen der Vektoren ist.
\subsection{Der Gradient zeigt immer in die Richtung des stärksten Anstiegs}
Suchen wir nun eine Richtung ${\overrightarrow{r}}$ in welche der Anstieg von einem fixen Punkt $a$ einer Funktion $f(x,y)$  maximal wird, also das Maximum von $f'(a){\overrightarrow{r}}$ für alle ${\overrightarrow{r}}$ mit $|{\overrightarrow{r}}|=1$. Nun kommt die vorhin erklärte Ungleichung ins Spiel:
$f'(a){\overrightarrow{r}}={\langle}{\nabla}f(a),r{\rangle} {\leq} |{\langle}{\nabla}f(a),{\overrightarrow{r}}{\rangle}| {\leq} |{\nabla}f(a)|{\cdot}|{\overrightarrow{r}}|=|{\nabla}f(a)|$
Falls der Gradient nun ungleich null ist und damit Gleichheit herrscht, müssen ${\overrightarrow{r}}$ und der Gradient linear abhängig sein, also: ${\overrightarrow{r}}={\lambda}*{\nabla}f(a)$. Das wiederrum bedeutetet, dass ${\overrightarrow{r}}=\frac{{\nabla}f(a)}{|{\nabla}f(a)|}$, dem ensprechend zeigt ${\overrightarrow{r}}$, bei welchem wir ja definiert haben, dass es in die Richtung des stärksten Anstiegs zeigt, in die gleiche Richtung wie der Gradient.

\subsection{Der Gradient steht im Rechten Winkel auf die Höhenschichtlinien}
Nun sagen wir es gibt die Richtung ${\overrightarrow{v}}$ in welcher der Anstieg gleich null ist, also die Richtung, in welche die Höhenschichtlinie wandert. Dem entsprechend muss das Skalarprodukt von ${\overrightarrow{v}}$ und ${\nabla}f(a)$ gleich null sein. Da ${\langle}{\nabla}f(a),r{\rangle} = |{\nabla}f(a)| * cos({\phi})$ muss ${\phi}$, also der Winkel zwischen dem Gradienten und der Höhenschichtlinie, damit die Gleichung stimmt gleich $90$ grad sein. Das heißt der Gradient steht immer im rechten Winkel auf die Höhenschichtlinien.

\subsection{Die Gradienten stehen beim normalen Abstiegsverfahren immer normal aufeinander}
Nun, das ist eine absolut logische Schlussfolgerung auf das bisher gezeigte. Beim normalen Abstiegsverfahren wandert man solange in die Richtung des Gradienten bis die erste Ableitung der eindimensionalen Funktion in welche man wandert gleich null ist. D.h. die Richtungsänderung der Richtung in welche man wandert ist gleich null. Da in die Richtung, in welcher die Steigung gleich null ist die Höhenschichtlinie verläuft und der Gradient immer rechtwinkelig auf der Höhenschichtlinie steht, steht der Gradient von diesem Punkt normal auf der Richtung aus der wir kommen.




\end{document}